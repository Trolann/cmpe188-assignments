{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Worked with:\n",
    "\n",
    "- Trevor Mathisen\n",
    "- Viet Nguyen"
   ],
   "id": "dbff4a4b0050da5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Redo MLR HW with Boston dataset",
   "id": "a4431033ec09775e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pandas import read_csv, DataFrame, Series\n",
    "from pandas.plotting import scatter_matrix\n",
    "from numpy import set_printoptions, argmax, isnan, nan, mean, random\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "filename = 'boston.csv'\n",
    "data = read_csv(filename)\n",
    "set_printoptions(precision=3)\n",
    "data = data.drop('index', axis=1)\n",
    "print(data.head(5))\n",
    "print(data.isnull().sum())\n",
    "print(data.shape)\n",
    "# Display unique values in each column\n",
    "for col in data.columns:\n",
    "    unique_values = data[col].unique()\n",
    "    print(f\"Unique values in '{col}': {unique_values}\")"
   ],
   "id": "33c3cd780be9c29d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data.hist()\n",
    "plt.suptitle(f\"Histograms of Data\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "589aea1e23a13544",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "features_to_standardize = ['rm', 'ptratio', 'dis', 'nox', 'tax', 'lstat']\n",
    "features_to_normalize = ['crim', 'zn', 'indus', 'age', 'rad', 'black']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), features_to_standardize),\n",
    "        ('norm', Normalizer(), features_to_normalize)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "X = data.drop('medv', axis=1)\n",
    "y = data['medv']\n",
    "X_processed = preprocessing_pipeline.fit_transform(X)"
   ],
   "id": "2fdd4a5f6110c6f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_features = features_to_standardize + features_to_normalize + ['chas']\n",
    "\n",
    "# Convert X_processed back to a DataFrame\n",
    "X_processed = DataFrame(X_processed, columns=all_features)\n",
    "X_processed.hist()\n",
    "plt.suptitle(\"Histograms of Processed Data\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "131b49cbc2ecef05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure() # new plot\n",
    "#plt.tight_layout()\n",
    "corMat = X_processed.corr(method='pearson')\n",
    "print(corMat)\n",
    "## plot correlation matrix as a heat map\n",
    "sns.heatmap(corMat, square=True)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(f\"CORRELATION MATRIX USING HEAT MAP\")\n",
    "plt.show()\n",
    "\n",
    "## scatter plot of all data\n",
    "plt.figure()\n",
    "# # The output overlaps itself, resize it to display better (w padding)\n",
    "scatter_matrix(X_processed)\n",
    "plt.tight_layout(pad=0.1)\n",
    "plt.show()"
   ],
   "id": "1a7068530bb56360",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)",
   "id": "299d2d734edcbec7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def determine_optimal_number_of_features(X, y):\n",
    "    feature_counts = range(1, X.shape[1] + 1)\n",
    "    scores = []\n",
    "    print(feature_counts)\n",
    "    for num_features in feature_counts:\n",
    "        model = LinearRegression()\n",
    "        rfe = RFE(model, n_features_to_select = num_features)\n",
    "        fit = rfe.fit(X, y)\n",
    "        print(\"Num Features:\", fit.n_features_)\n",
    "        print(\"Selected Features:\", fit.support_)\n",
    "        print(\"Feature Ranking:\", fit.ranking_)\n",
    "        scores.append(rfe.score(X,y))\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(feature_counts, scores, 'b-', marker='o')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.title('Model Performance vs Number of Features')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    best_num_features = feature_counts[argmax(scores)]\n",
    "    print(f\"Optimal number of features: {best_num_features}\")\n",
    "    print(f\"Best score: {max(scores):.4f}\")\n",
    "    print(f\"Features selected: {X.columns[fit.support_]}\")\n",
    "    return feature_counts, scores\n",
    "\n",
    "feature_counts, scores = determine_optimal_number_of_features(X_train, y_train)"
   ],
   "id": "74dc03a30f705d93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def determine_features_within_threshold(_feature_counts, _scores):\n",
    "    best_score = max(_scores)\n",
    "    best_score_index = argmax(_scores)\n",
    "    threshold = best_score * 0.98\n",
    "    best_num_features = _feature_counts[best_score_index]\n",
    "    i = best_score_index\n",
    "    for i in range(best_score_index, 0, -1):\n",
    "        if _scores[i] < threshold:\n",
    "            print(f\"Lowest number of features within 1% of best score: {_feature_counts[i]}\")\n",
    "            break\n",
    "print(\"Raw Data\")\n",
    "determine_features_within_threshold(feature_counts, scores)"
   ],
   "id": "3de4e4eef161ebf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def stepwise_selection(X, y,\n",
    "                       initial_list=[],\n",
    "                       threshold_in=0.01,\n",
    "                       threshold_out = 0.05,\n",
    "                       verbose=True):\n",
    "        \"\"\" Perform a forward-backward feature selection\n",
    "        based on p-value from statsmodels.api.OLS\n",
    "        Arguments:\n",
    "            X - pandas.DataFrame with candidate features\n",
    "            y - list-like with the target\n",
    "            initial_list - list of features to start with (column names of X)\n",
    "            threshold_in - include a feature if its p-value < threshold_in\n",
    "            threshold_out - exclude a feature if its p-value > threshold_out\n",
    "            verbose - whether to print the sequence of inclusions and exclusions\n",
    "        Returns: list of selected features\n",
    "        Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "        See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "        \"\"\"\n",
    "        included = list(initial_list)\n",
    "        while True:\n",
    "            changed=False\n",
    "            # forward step\n",
    "            excluded = list(set(X.columns)-set(included))\n",
    "            new_pval = Series(index=excluded)\n",
    "            for new_column in excluded:\n",
    "                model = sm.OLS(y, sm.add_constant(DataFrame(X[included+[new_column]]))).fit()\n",
    "                new_pval[new_column] = model.pvalues[new_column]\n",
    "            best_pval = new_pval.min()\n",
    "            if best_pval < threshold_in:\n",
    "                best_feature = new_pval.idxmin()\n",
    "                included.append(best_feature)\n",
    "                changed=True\n",
    "                if verbose:\n",
    "                    print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "            # backward step\n",
    "            model = sm.OLS(y, sm.add_constant(DataFrame(X[included]))).fit()\n",
    "            # use all coefs except intercept\n",
    "            pvalues = model.pvalues.iloc[1:]\n",
    "            worst_pval = pvalues.max() # null if pvalues is empty\n",
    "            if worst_pval > threshold_out:\n",
    "                changed=True\n",
    "                worst_feature = pvalues.idxmax()\n",
    "                included.remove(worst_feature)\n",
    "                if verbose:\n",
    "                    print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "            if not changed:\n",
    "                break\n",
    "        return included"
   ],
   "id": "d6e495bf5166f2f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result = stepwise_selection(X_train, y_train)",
   "id": "200f1dbe309416cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build a multiple linear regression model using the RFE and the stepwise methods.",
   "id": "33ad1e6543ac6ce8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build a multiple linear regression model using the RFE and the stepwise methods.\n",
    "# RFE\n",
    "rfe_model = LinearRegression()\n",
    "rfe = RFE(rfe_model, n_features_to_select=8)\n",
    "fit = rfe.fit(X_train, y_train)\n",
    "selected_columns = X_train.columns[fit.support_]\n",
    "X_train_rfe = X_train[selected_columns]\n",
    "rfe_model.fit(X_train_rfe, y_train)\n",
    "print(\"Num Features:\", fit.n_features_)\n",
    "print(\"Selected Features:\", fit.support_)\n",
    "print(\"Feature Ranking:\", fit.ranking_)\n",
    "\n",
    "# Stepwise\n",
    "step_model = LinearRegression()\n",
    "# Build a model using the selected features in result\n",
    "X_train_step = X_train[result]\n",
    "step_model.fit(X_train_step, y_train)\n",
    "print(\"Num Features:\", len(result))\n",
    "print(\"Selected Features:\", result)\n",
    "print(\"Feature Ranking:\", [X_train.columns.get_loc(x) for x in result])\n",
    "\n",
    "# Compare the two models across the training and test sets\n",
    "def compare_models(model1, model2, X_test_rfe, X_test_step, y_test):\n",
    "    # Calculate R² score for both models\n",
    "    r2_score1 = model1.score(X_test_rfe, y_test)\n",
    "    r2_score2 = model2.score(X_test_step, y_test)\n",
    "\n",
    "    print(f\"RFE Model R² Score: {r2_score1:.4f}\")\n",
    "    print(f\"Stepwise Model R² Score: {r2_score2:.4f}\")\n",
    "\n",
    "# Prepare test data with the correct features for each model\n",
    "X_test_rfe = X_test[selected_columns]  # Only use columns selected by RFE\n",
    "X_test_step = X_test[result]           # Only use columns selected by stepwise\n",
    "\n",
    "# Compare models with appropriate test data\n",
    "compare_models(rfe_model, step_model, X_test_rfe, X_test_step, y_test)"
   ],
   "id": "f9e36566f9a82337",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use the Boston dataset and design a regression model using MLP regressor.\n",
    "\n",
    "Compare the results with MLR model using cross validation. (rerun with MLR)\n",
    "\n",
    "Comment on the MLP regressor architecture and its relationship with overfitting."
   ],
   "id": "22df37ee156468ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set random seeds for reproducibility\n",
    "tf.keras.backend.clear_session()\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "data = read_csv(filename)\n",
    "data = data.drop('index', axis=1)\n",
    "X = data.drop('medv', axis=1)\n",
    "y = data['medv']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)"
   ],
   "id": "ea2bcb9889d7a998",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_layer = tf.keras.layers.Input(shape=(X_train.shape[1],))\n",
    "#hidden1 = tf.keras.layers.Dense(64, activation=\"relu\")(input_layer)\n",
    "hidden2 = tf.keras.layers.Dense(32, activation=\"relu\", kernel_initializer='he_normal')(input_layer)#hidden1)\n",
    "#hidden2 = tf.keras.layers.Dense(32, activation='selu', kernel_initializer='lecun_normal')(input_layer)\n",
    "output = tf.keras.layers.Dense(1)(hidden2)  # No activation for regression\n",
    "model = tf.keras.models.Model(inputs=[input_layer], outputs=[output])\n",
    "\n",
    "# import SGD optimizer to use momentum\n",
    "#sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "sgd = 'sgd'\n",
    "model.compile(\n",
    "    loss=\"mean_squared_error\",\n",
    "    optimizer=sgd,\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "model.summary()"
   ],
   "id": "f055025cd40ba6c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    #epochs=100,  # Adjusted based on the later results and the output of 100 epochs showing diminishing returns\n",
    "    epochs=50,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "331776d4bc025000",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[101], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#epochs=100,  # Adjusted based on the later results and the output of 100 epochs showing diminishing returns\u001B[39;49;00m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_valid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_valid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/cmpe188/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/cmpe188/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:395\u001B[0m, in \u001B[0;36mTensorFlowTrainer.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_eval_epoch_iterator\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eval_epoch_iterator \u001B[38;5;241m=\u001B[39m TFEpochIterator(\n\u001B[1;32m    386\u001B[0m         x\u001B[38;5;241m=\u001B[39mval_x,\n\u001B[1;32m    387\u001B[0m         y\u001B[38;5;241m=\u001B[39mval_y,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    393\u001B[0m         shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    394\u001B[0m     )\n\u001B[0;32m--> 395\u001B[0m val_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_x\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_sample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_batch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_use_cached_eval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    405\u001B[0m val_logs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    406\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m name: val \u001B[38;5;28;01mfor\u001B[39;00m name, val \u001B[38;5;129;01min\u001B[39;00m val_logs\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m    407\u001B[0m }\n\u001B[1;32m    408\u001B[0m epoch_logs\u001B[38;5;241m.\u001B[39mupdate(val_logs)\n",
      "File \u001B[0;32m~/cmpe188/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/cmpe188/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:481\u001B[0m, in \u001B[0;36mTensorFlowTrainer.evaluate\u001B[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m    479\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset_metrics()\n\u001B[1;32m    480\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m epoch_iterator\u001B[38;5;241m.\u001B[39mcatch_stop_iteration():\n\u001B[0;32m--> 481\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterator\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mepoch_iterator\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_test_batch_begin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/cmpe188/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:734\u001B[0m, in \u001B[0;36mTFEpochIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    733\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_epoch_iterator\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/cmpe188/.venv/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:102\u001B[0m, in \u001B[0;36mEpochIterator._enumerate_iterator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m steps_per_epoch \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps_per_epoch \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 102\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    103\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_seen \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, steps_per_epoch, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps_per_execution):\n",
      "File \u001B[0;32m~/cmpe188/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py:501\u001B[0m, in \u001B[0;36mDatasetV2.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly() \u001B[38;5;129;01mor\u001B[39;00m ops\u001B[38;5;241m.\u001B[39minside_function():\n\u001B[1;32m    500\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mcolocate_with(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variant_tensor):\n\u001B[0;32m--> 501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43miterator_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mOwnedIterator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    503\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tf.data.Dataset` only supports Python-style \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    504\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miteration in eager mode or within tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/cmpe188/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/iterator_ops.py:709\u001B[0m, in \u001B[0;36mOwnedIterator.__init__\u001B[0;34m(self, dataset, components, element_spec)\u001B[0m\n\u001B[1;32m    705\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m (components \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m element_spec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot be specified.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 709\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    711\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_next_call_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/cmpe188/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/iterator_ops.py:748\u001B[0m, in \u001B[0;36mOwnedIterator._create_iterator\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    745\u001B[0m   \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(fulltype\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39margs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(\n\u001B[1;32m    746\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_output_types)\n\u001B[1;32m    747\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator_resource\u001B[38;5;241m.\u001B[39mop\u001B[38;5;241m.\u001B[39mexperimental_set_type(fulltype)\n\u001B[0;32m--> 748\u001B[0m \u001B[43mgen_dataset_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_iterator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds_variant\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator_resource\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/cmpe188/.venv/lib/python3.12/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3478\u001B[0m, in \u001B[0;36mmake_iterator\u001B[0;34m(dataset, iterator, name)\u001B[0m\n\u001B[1;32m   3476\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[1;32m   3477\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3478\u001B[0m     _result \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_FastPathExecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3479\u001B[0m \u001B[43m      \u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mMakeIterator\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3480\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[1;32m   3481\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot learning curves\n",
    "DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "mlp_mse = mean_squared_error(y_test, y_pred)\n",
    "mlp_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MLP Test MSE: {mlp_mse:.4f}\")\n",
    "print(f\"MLP Test MAE: {test_mae:.4f}\")\n",
    "print(f\"MLP Test R²: {mlp_r2:.4f}\")\n",
    "\n",
    "# Compare with Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "lr_mse = mean_squared_error(y_test, y_pred_lr)\n",
    "lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Linear Regression Test MSE: {lr_mse:.4f}\")\n",
    "print(f\"Linear Regression Test R²: {lr_r2:.4f}\")"
   ],
   "id": "da44f0fec87eb2f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weights, biases = model.layers[1].get_weights()\n",
    "print(\"First layer weights shape:\", weights.shape)\n",
    "print(\"First layer biases shape:\", biases.shape)\n",
    "\n",
    "# Plot the first few weights to visualize what the model learned\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(weights.flatten(), bins=50)\n",
    "plt.title(\"Distribution of Weights in First Layer\")\n",
    "plt.xlabel(\"Weight Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(biases, bins=20)\n",
    "plt.title(\"Distribution of Biases in First Layer\")\n",
    "plt.xlabel(\"Bias Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"MLP Regressor: Actual vs Predicted Values\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c4ecb5100c29a370",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
