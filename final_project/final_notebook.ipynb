{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Final Notebook\n",
    "# Created by: [Aaron Ooi, Viet Nguyen, Trevor Mathisen]"
   ],
   "id": "36ffc227b55e8496"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports and setup",
   "id": "15332ee873c0d243"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_models = [] # Add your final models here as a tuple of ('name', model)) for Part 3\n",
    "%load_ext cuml.accel"
   ],
   "id": "f0f508524da4fb84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy import ndarray, argsort, set_printoptions, argmax, isnan, nan, mean, random\n",
    "\n",
    "from pandas import set_option, read_csv, DataFrame, concat, Series\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, f1_score, roc_auc_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import auc as auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from cuml.svm import SVC as cuSVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, roc_curve\n",
    "from sklearn.metrics import auc as auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cudf\n",
    "from cuml.metrics import accuracy_score\n",
    "import cupy as cp\n",
    "# Timing\n",
    "\n"
   ],
   "id": "8a062c946ac9dc94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 1: EDA",
   "id": "50abb9c404f7102"
  },
  {
   "cell_type": "code",
   "id": "129b99ef1f43feaf",
   "metadata": {},
   "source": [
    "DONT_RUN_LONG_CELLS = True\n",
    "kickstarter_filename = 'kickstarter_data_full.csv'\n",
    "kickstarter_filename_features = 'kickstarter_data_with_features.csv'\n",
    "\n",
    "ks_data = read_csv(kickstarter_filename)\n",
    "ks_feat_data = read_csv(kickstarter_filename_features)\n",
    "data_list = [('ks_data', ks_data), ('ks_feat_data', ks_feat_data)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Determine if the two datasets are the same",
   "id": "d556a20273f41afd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print columns not present in the other dataset\n",
    "ks_data_columns = set(ks_data.columns)\n",
    "ks_feat_data_columns = set(ks_feat_data.columns)\n",
    "\n",
    "ks_data_not_in_feat = ks_data_columns - ks_feat_data_columns\n",
    "ks_feat_data_not_in_ks = ks_feat_data_columns - ks_data_columns\n",
    "\n",
    "print(f'Columns in ks_data not in ks_feat_data: {ks_data_not_in_feat}')\n",
    "print(f'Columns in ks_feat_data not in ks_data: {ks_feat_data_not_in_ks}')\n",
    "\n",
    "common_columns = ks_data_columns.intersection(ks_feat_data_columns)\n",
    "\n",
    "# of common columns, compare the values and see if they match\n",
    "for column in common_columns:\n",
    "    ks_data_values = ks_data[column].unique()\n",
    "    ks_feat_data_values = ks_feat_data[column].unique()\n",
    "    if len(ks_data_values) != len(ks_feat_data_values):\n",
    "        print(f'Column {column} has different number of unique values: {len(ks_data_values)} vs {len(ks_feat_data_values)}')\n",
    "    else:\n",
    "        pass"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## So we know ks_data is the dataset to use as it has more data, more unique data\n",
   "id": "a215b9ee9619ff2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_list = [('ks_data', ks_data), ('ks_feat_data', ks_feat_data)]\n",
    "\n",
    "def data_info(_data, show_scatter=False):\n",
    "    print(f'Data head: {_data.head(5)}')\n",
    "    print(f'Null values: {_data.isnull().sum()}')\n",
    "    print(f'Data Shape: {_data.shape[0]} rows and {_data.shape[1]} columns')\n",
    "    print(f'Columns: {list(_data.columns)}')\n",
    "    for column in _data.columns:\n",
    "        if len(_data[column].unique()) < 10:\n",
    "            print(f'{column} unique values: {_data[column].unique()}')\n",
    "        else:\n",
    "            percent_unique = len(_data[column].unique()) / _data.shape[0] * 100\n",
    "            print(f'{column} % unique values: {percent_unique}')\n",
    "    print(_data.describe())\n",
    "    _data.hist(figsize=(12, 10))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.figure()  # new plot\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Only show for floats and ints\n",
    "\n",
    "    float_columns = _data.select_dtypes(include=['float64']).columns\n",
    "    int_columns = _data.select_dtypes(include=['int64']).columns\n",
    "    # Combine float and int columns\n",
    "\n",
    "    numeric_columns = float_columns.append(int_columns)\n",
    "    # Calculate correlation matrix\n",
    "    corMat = _data[numeric_columns].corr(method='pearson')\n",
    "\n",
    "    print(corMat)\n",
    "    ## plot correlation matrix as a heat map\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(corMat, square=True)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f\"CORRELATION MATRIX USING HEAT MAP\")\n",
    "    plt.show()\n",
    "\n",
    "    ## scatter plot of all _data\n",
    "    plt.figure()\n",
    "    # # The output overlaps itself, resize it to display better (w padding)\n",
    "    if show_scatter:\n",
    "        try:\n",
    "            scatter_matrix(_data)\n",
    "            plt.tight_layout(pad=0.1)\n",
    "            plt.show()\n",
    "        except:\n",
    "            return\n",
    "\n",
    "\n",
    "def data_info_less(_date):\n",
    "    set_option('display.max_columns', None)\n",
    "    if not isinstance(_date, list):\n",
    "        if not isinstance(_date, tuple):\n",
    "            _date = ('', _date)\n",
    "        _date = [_date]\n",
    "    for name, data in _date:\n",
    "        print(f'{name} data')\n",
    "        print(data.info())\n",
    "        print(data.head(5))\n",
    "        for column in data.columns:\n",
    "            highlight_column = 'profile'\n",
    "            if column == highlight_column:\n",
    "                # print 2 rows of values completely\n",
    "                row1 = data.iloc[0][highlight_column]\n",
    "                row2 = data.iloc[1][highlight_column]\n",
    "                print(f'Row 1: {row1}')\n",
    "                print(f'Row 2: {row2}')\n",
    "            if len(data[column].unique()) < 10:\n",
    "                print(f'{column} unique values: {data[column].unique()}')\n",
    "            else:\n",
    "                percent_unique = len(data[column].unique()) / data.shape[0] * 100\n",
    "                print(f'{column} % unique values: {percent_unique}')\n",
    "        break"
   ],
   "id": "2b93bdd151388141",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_info(ks_data)\n",
   "id": "2c2b6d1ef8612722",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### By iteratively looking at columns in this cell, and looking at just those not categorized by a human, we can determine what columns are not needed, and types of others\n",
    "\n",
    "### We also determine unique columns and their values to setup for one-hot encoding later"
   ],
   "id": "5d287dec4c507ec2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns_to_drop = ['Unnamed: 0', 'id', 'photo', 'name', 'blurb', 'slug', 'currency_symbol', 'currency_trailing_code', 'static_usd_rate', 'creator', 'profile', 'friends', 'is_backing', 'permissions', 'name_len', 'blurb_len', 'urls', 'source_url', 'location', 'is_starred', 'create_to_launch']\n",
    "float_columns = ['goal', 'pledged', 'usd_pledged']\n",
    "int_columns = ['backers_count', 'name_len_clean', 'blurb_len_clean',  'launch_to_deadline', 'launch_to_state_change', 'create_to_launch_days', 'launch_to_deadline_days', 'launch_to_state_change_days', ]\n",
    "datetime_columns = ['deadline', 'state_changed_at', 'created_at', 'launched_at']\n",
    "date_int_columns = ['deadline_month', 'deadline_day', 'deadline_hr', 'state_changed_at_month', 'state_changed_at_day', 'state_changed_at_month', 'state_changed_at_day', 'state_changed_at_yr', 'created_at_month', 'created_at_day', 'created_at_hr', 'launched_at_month', 'launched_at_day', 'launched_at_hr', 'state_changed_at_hr', 'created_at_yr', 'launched_at_yr']\n",
    "category_columns = ['state', 'currency', 'staff_pick', 'category', 'deadline_weekday', 'state_changed_at_weekday', 'created_at_weekday', 'launched_at_weekday', 'deadline_yr', 'country']\n",
    "boolean_columns = ['disable_communication', 'spotlight', 'SuccessfulBool', 'USorGB', 'TOPCOUNTRY', 'LaunchedTuesday', 'DeadlineWeekend']\n",
    "\n",
    "# This is just to categorize everything and make sure I'm not missing anything\n",
    "temp_data = ks_data.copy()\n",
    "data_info_less(temp_data)\n",
    "temp_data = temp_data.drop(columns=columns_to_drop)\n",
    "temp_data = temp_data.drop(columns=datetime_columns)\n",
    "temp_data = temp_data.drop(columns=category_columns)\n",
    "temp_data = temp_data.drop(columns=boolean_columns)\n",
    "temp_data = temp_data.drop(columns=float_columns)\n",
    "temp_data = temp_data.drop(columns=int_columns)\n",
    "temp_data = temp_data.drop(columns=date_int_columns)\n",
    "print('\\n' * 3)\n",
    "print(f'Lets see what we have left')\n",
    "data_info_less(('temp_data', temp_data))"
   ],
   "id": "e401b3fdfdaff299",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Examine and clean columns",
   "id": "b1c8d0d0f80e971c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kickstarter = ks_data.copy()\n",
    "#kickstarter = kickstarter.dropna()\n",
    "kickstarter = kickstarter.drop(columns=columns_to_drop)\n",
    "# Specifically examine the problematic columns\n",
    "print(\"NaN in name_len_clean:\", kickstarter['name_len_clean'].isna().sum())\n",
    "print(\"NaN in blurb_len_clean:\", kickstarter['blurb_len_clean'].isna().sum())\n",
    "\n",
    "print(kickstarter['category'].unique())\n",
    "# total number of records\n",
    "print(kickstarter.shape)\n",
    "# Number of na's in 'category'\n",
    "category_nas = kickstarter['category'].isna().sum()\n",
    "total_rows = kickstarter.shape[0]\n",
    "print(f'Category has {(category_nas / total_rows * 100):.2f}% as \"nan\"')\n",
    "temp_data = ks_data.copy()\n",
    "# Drop the 10 rows which have _len_clean as nan\n",
    "kickstarter = kickstarter.dropna(subset=['name_len_clean', 'blurb_len_clean'])\n",
    "print(\"NaN in name_len_clean:\", kickstarter['name_len_clean'].isna().sum())\n",
    "print(\"NaN in blurb_len_clean:\", kickstarter['blurb_len_clean'].isna().sum())\n",
    "\n",
    "kickstarter['name_len_clean'] = kickstarter['name_len_clean'].astype(int)\n",
    "kickstarter['blurb_len_clean'] = kickstarter['blurb_len_clean'].astype(int)\n",
    "\n",
    "# Replace 'nan' in category with 'None' (string not object), then do one-hot encoding\n",
    "kickstarter['category'] = kickstarter['category'].fillna(\"None\")\n",
    "print(kickstarter['category'].unique())"
   ],
   "id": "7cb1cd08c8b5b53a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### One-hot encode the categorical columns",
   "id": "2e7b88c2e4c00e16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def encode_categorical_features(df: DataFrame) -> tuple:\n",
    "    \"\"\"Encode all categorical features in the dataframe\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "\n",
    "    # Get all categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    print(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "    # Handle boolean columns separately\n",
    "    boolean_columns = ['staff_pick']\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace({False: 0, True: 1})\n",
    "            print(f\"Converted boolean column: {col}\")\n",
    "\n",
    "    # Columns to one-hot encode\n",
    "    columns_to_encode = [\n",
    "        'category',\n",
    "        'deadline_weekday',\n",
    "        'created_at_weekday',\n",
    "        'launched_at_weekday'\n",
    "    ]\n",
    "\n",
    "    # Filter out columns that aren't in the dataframe\n",
    "    columns_to_encode = [col for col in columns_to_encode if col in df.columns]\n",
    "\n",
    "    # Remove 'state_changed_at_weekday' column if it exists\n",
    "    if 'state_changed_at_weekday' in df.columns:\n",
    "        df = df.drop(columns='state_changed_at_weekday')\n",
    "        print(\"Dropped 'state_changed_at_weekday' column\")\n",
    "\n",
    "    # One-hot encode the selected columns\n",
    "    all_encoded_dfs = []\n",
    "\n",
    "    for col in columns_to_encode:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded = encoder.fit_transform(df[[col]])\n",
    "\n",
    "        # Create meaningful column names\n",
    "        encoded_cols = [f\"{col}_{c}\" for c in encoder.categories_[0]]\n",
    "        encoded_df = DataFrame(encoded, columns=encoded_cols, index=df.index)\n",
    "\n",
    "        # Add to list of encoded dataframes\n",
    "        all_encoded_dfs.append(encoded_df)\n",
    "\n",
    "        # Drop the original column\n",
    "        print(f'Dropping original column: {col}')\n",
    "        df = df.drop(columns=col)\n",
    "        print(f\"Encoded column: {col} → {len(encoded_cols)} features\")\n",
    "\n",
    "    # Combine all dataframes\n",
    "    result = concat([df] + all_encoded_dfs, axis=1)\n",
    "\n",
    "    # Create a list of all categorical columns (original and encoded)\n",
    "    all_categorical_columns = []\n",
    "    for col in categorical_columns:\n",
    "        if col in result.columns:\n",
    "            all_categorical_columns.append(col)\n",
    "        else:\n",
    "            # Add the encoded column names\n",
    "            all_categorical_columns.extend([c for c in result.columns if c.startswith(f\"{col}_\")])\n",
    "\n",
    "    print(f\"Total categorical columns after encoding: {len(all_categorical_columns)}\")\n",
    "\n",
    "    return result, all_categorical_columns\n",
    "\n",
    "# Example usage\n",
    "kickstarter, category_columns = encode_categorical_features(kickstarter)"
   ],
   "id": "5d41b13369c4fc60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Now we know what is in the data, let's look at some of the features and see if we can find any interesting patterns.",
   "id": "a4e025ef48ae0079"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For each feature, lets see a histogram of the values and a boxplot of the values\n",
    "def plot_feature_distribution(data, feature):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data[feature], bins=30, kde=True)\n",
    "    plt.title(f'{feature} Distribution')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=data[feature])\n",
    "    plt.title(f'{feature} Boxplot')\n",
    "    plt.xlabel(feature)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "columns_to_show = float_columns + int_columns\n",
    "\n",
    "for feature in kickstarter.columns:\n",
    "    if feature in columns_to_show and not DONT_RUN_LONG_CELLS:\n",
    "        plot_feature_distribution(kickstarter, feature)"
   ],
   "id": "bdb079561b04b902",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Based on the above, perform scaling on the float and int columns",
   "id": "5c8c1bb12c2bcc3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns_to_standardize = [ # Roughly normal\n",
    "    'name_len_clean', 'blurb_len_clean',\n",
    "    'deadline_month', 'deadline_day', 'deadline_hr',\n",
    "    'state_changed_at_month', 'state_changed_at_day', 'state_changed_at_hr',\n",
    "    'created_at_month', 'created_at_day', 'created_at_hr',\n",
    "    'launched_at_month', 'launched_at_day', 'launched_at_hr'\n",
    "]\n",
    "\n",
    "columns_to_log_transform = [ # Heavily right skewed\n",
    "    'goal', 'pledged', 'usd_pledged', 'backers_count',\n",
    "    'create_to_launch_days'\n",
    "]\n",
    "\n",
    "columns_to_normalize = [ # Different scales\n",
    "    'launch_to_deadline_days', 'launch_to_state_change_days'\n",
    "]\n",
    "\n",
    "columns_to_drop2 = [\n",
    "    # Temporal columns represented by better features\n",
    "    'deadline', 'state_changed_at', 'created_at', 'launched_at',\n",
    "    'launch_to_deadline', 'launch_to_state_change',\n",
    "\n",
    "    # Duplicate/redundant information\n",
    "    'deadline_yr', 'state_changed_at_yr', 'created_at_yr', 'launched_at_yr',\n",
    "\n",
    "    # Categorical with many unique values, better represented by other features\n",
    "    'country', 'currency',\n",
    "\n",
    "    # Low variance or binary columns that might be redundant\n",
    "    'disable_communication', 'spotlight',\n",
    "    'USorGB', 'TOPCOUNTRY', 'LaunchedTuesday', 'DeadlineWeekend'\n",
    "]\n",
    "\n",
    "# Drop the columns we don't want\n",
    "kickstarter = kickstarter.drop(columns=columns_to_drop2)\n",
    "print(kickstarter.columns)\n",
    "\n",
    "for column in columns_to_log_transform:\n",
    "    print(f\"NaN values in '{column}': {kickstarter[column].isna().sum()}\")\n",
    "\n",
    "print(kickstarter.shape[0])\n",
    "kickstarter = kickstarter.dropna(subset=columns_to_log_transform)\n",
    "print(kickstarter.shape[0])\n",
    "\n",
    "# Standardize the columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "for column in columns_to_standardize:\n",
    "    kickstarter[column] = scaler.fit_transform(kickstarter[[column]])\n",
    "\n",
    "# Log transform the columns\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "for column in columns_to_log_transform:\n",
    "    kickstarter[column] = log_transformer.fit_transform(kickstarter[[column]])\n",
    "# Normalize the columns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalizer = MinMaxScaler()\n",
    "for column in columns_to_normalize:\n",
    "    kickstarter[column] = normalizer.fit_transform(kickstarter[[column]])"
   ],
   "id": "eedc3850cfbfec56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check the data again\n",
    "data_info(kickstarter)"
   ],
   "id": "e676dba30f389765",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### We need to make a 'success' classifier, so lets take a look at the definition of success in the data",
   "id": "529d37a99f0cd60b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compare 'state' and 'SuccessfulBool' to see if they are the same\n",
    "print(ks_data['state'].unique())\n",
    "print(ks_data['SuccessfulBool'].unique())\n",
    "print(ks_data['state'].value_counts())\n",
    "print(ks_data['SuccessfulBool'].value_counts())\n",
    "# Make a table where the rows are 0/1 for SuccessfulBool and the columns are the states\n",
    "print(ks_data.groupby(['SuccessfulBool', 'state']).size().unstack())\n",
    "\n",
    "# Show the distribution of 'pledged/goal' to see if we can make a column called 'got_funded'\n",
    "temp_data = ks_data.copy()\n",
    "\n",
    "# confirm they're both numbers\n",
    "print(temp_data['pledged'].dtype)\n",
    "print(temp_data['goal'].dtype)\n",
    "\n",
    "# show the count where pledged > goal\n",
    "print(f'{temp_data[temp_data['pledged'] > temp_data['goal']].shape[0]} projects got funded')\n",
    "\n",
    "# Create got_funded column and show the histogram\n",
    "temp_data['got_funded'] = temp_data['pledged'] / temp_data['goal']\n",
    "\n",
    "# Describe the column\n",
    "print(temp_data['got_funded'].describe())\n",
    "\n",
    "# Get count of SuccessfulBool\n",
    "print(temp_data['SuccessfulBool'].value_counts())\n",
    "# Shows the same thing\n",
    "\n",
    "# using kickstarter data, show some good plots of features vs SuccessfulBool\n",
    "\n",
    "def plot_feature_vs_success(data, feature):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data[data['SuccessfulBool'] == 1][feature], bins=30, kde=True, color='green', label='Successful')\n",
    "    sns.histplot(data[data['SuccessfulBool'] == 0][feature], bins=30, kde=True, color='red', label='Failed')\n",
    "    plt.title(f'{feature} vs SuccessfulBool')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=data['SuccessfulBool'], y=data[feature])\n",
    "    plt.title(f'{feature} vs SuccessfulBool')\n",
    "    plt.xlabel('SuccessfulBool')\n",
    "    plt.ylabel(feature)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show the features that are not categorical\n",
    "for feature in kickstarter.columns:\n",
    "    if DONT_RUN_LONG_CELLS:\n",
    "        break\n",
    "    if feature not in ['state', 'SuccessfulBool', 'category', 'currency', 'staff_pick']:\n",
    "        plot_feature_vs_success(kickstarter, feature)"
   ],
   "id": "b16c4d81faef2fdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Because `state` captures the same information as `SuccessfulBool`, we can drop it\n",
    "### and because `got_funded` and `pledged` offer information obtained at the end of the campaign, we should drop them as well"
   ],
   "id": "a5cdde0e1bd53a84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kickstarter = kickstarter.drop(columns=['state', 'pledged', 'usd_pledged'])\n",
    "kickstarter['name_len_clean'] = kickstarter['name_len_clean'].astype(int)\n",
    "kickstarter['blurb_len_clean'] = kickstarter['blurb_len_clean'].astype(int)"
   ],
   "id": "ed6dd4ded6abf15d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kickstarter = kickstarter.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X = kickstarter.copy()\n",
    "Y1 = kickstarter['SuccessfulBool']\n",
    "X = X.drop(columns=['SuccessfulBool'])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y1, test_size=0.2, random_state=42, stratify=Y1)"
   ],
   "id": "1cdc8ae3317a1a92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 2: Models (tuning and final models)",
   "id": "3405667febbbcf8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model 1: Random Forest Classifier",
   "id": "6dbb439d78195f61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_rf_feature_importance(_X_train: DataFrame, _y_train: ndarray, n_features: int = 10) -> tuple:\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(_X_train, _y_train)\n",
    "\n",
    "    # Get feature importances\n",
    "    importances = rf.feature_importances_\n",
    "    indices = argsort(importances)[::-1]\n",
    "\n",
    "    # Select top n features\n",
    "    top_features = _X_train.columns[indices[:n_features]].tolist()\n",
    "    return top_features, indices\n",
    "\n",
    "\n",
    "def get_rfe_features(_X_train: DataFrame, _y_train: ndarray, n_features: int = 10) -> list:\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rfe = RFE(estimator=model, n_features_to_select=n_features)\n",
    "    rfe.fit(_X_train, _y_train)\n",
    "\n",
    "    # Get selected features\n",
    "    selected_features = _X_train.columns[rfe.support_].tolist()\n",
    "    return selected_features\n",
    "\n",
    "# Method 3: Statistical feature selection\n",
    "def get_statistical_features(_X_train: DataFrame, _y_train: ndarray, n_features: int = 10) -> list:\n",
    "    selector = SelectKBest(score_func=f_regression, k=n_features)\n",
    "    selector.fit(_X_train, _y_train)\n",
    "\n",
    "    # Get selected features\n",
    "    selected_mask = selector.get_support()\n",
    "    selected_features = _X_train.columns[selected_mask].tolist()\n",
    "    return selected_features\n",
    "\n",
    "# Get ordered features and indices from Random Forest\n",
    "top_features, importance_indices = get_rf_feature_importance(X_train, y_train, len(X_train.columns))\n",
    "ranked_features = X_train.columns[importance_indices].tolist()\n",
    "\n",
    "# Define range of feature counts to test\n",
    "feature_counts = [10, 20, 30, 40, 50, 60]\n",
    "if len(ranked_features) < 30:\n",
    "    feature_counts = list(range(5, len(ranked_features)+1, 5))\n",
    "\n",
    "# Test different feature counts\n",
    "cv_scores = []\n",
    "for n_features in feature_counts:\n",
    "    if n_features > len(ranked_features):\n",
    "        continue\n",
    "\n",
    "    # Select top n features\n",
    "    selected_features = ranked_features[:n_features]\n",
    "    X_selected = X_train[selected_features]\n",
    "\n",
    "    # Evaluate with cross-validation\n",
    "    scores = cross_val_score(\n",
    "        RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        X_selected, y_train,\n",
    "        cv=5, scoring='r2'\n",
    "    )\n",
    "\n",
    "    cv_scores.append(scores.mean())\n",
    "    print(f\"{n_features} features: R² = {scores.mean():.4f}\")\n",
    "    print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(feature_counts[:len(cv_scores)], cv_scores, marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cross-Validated R² Score')\n",
    "plt.title('Model Performance vs Number of Features')\n",
    "plt.grid(True)\n",
    "plt.savefig('feature_selection_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal number based on the results\n",
    "optimal_idx = max(range(len(cv_scores)), key=cv_scores.__getitem__)\n",
    "optimal_n_features = feature_counts[optimal_idx]\n",
    "print(f\"\\nOptimal number of features: {optimal_n_features}\")\n",
    "print(f\"Selected features: {ranked_features[:optimal_n_features]}\")\n",
    "\n",
    "\n",
    "print(f\"\\nRandom Forest selected {optimal_n_features} features: {ranked_features[:optimal_n_features]}\")\n",
    "\n",
    "\n",
    "optimal_features = ranked_features[:optimal_n_features]\n",
    "\n",
    "# Train the model using optimal features\n",
    "X_train_optimal = X_train[optimal_features]\n",
    "X_test_optimal = X_test[optimal_features]\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 15, 20, 25, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "base_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Setup Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search.fit(X_train_optimal, y_train)\n",
    "\n",
    "# Get best parameters and best model\n",
    "best_params = grid_search.best_params_\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_rf_model.predict(X_test_optimal)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Feature importance of the final model\n",
    "feature_importance = best_rf_model.feature_importances_\n",
    "importance_df = DataFrame({\n",
    "    'Feature': optimal_features,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(importance_df.head(10))  # Show top 10 features\n",
    "\n",
    "# Calculate cross-validation score for the best model\n",
    "cv_scores = cross_val_score(\n",
    "    best_rf_model,\n",
    "    X[optimal_features], Y1,\n",
    "    cv=5, scoring='accuracy'\n",
    ")\n",
    "print(f\"\\nCross-validation accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "y_pred_proba = best_rf_model.predict_proba(X_test_optimal)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc_score(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2,\n",
    "         label=f'ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize hyperparameter tuning results\n",
    "results = DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Plot accuracy for different max_depth values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Filter results for default min_samples values\n",
    "default_samples = results[(results['param_min_samples_split'] == 2) &\n",
    "                         (results['param_min_samples_leaf'] == 1)]\n",
    "\n",
    "# Convert None to -1 for plotting purposes\n",
    "default_samples['param_max_depth'] = default_samples['param_max_depth'].fillna(-1)\n",
    "\n",
    "# Sort by max_depth for proper x-axis order\n",
    "default_samples = default_samples.sort_values('param_max_depth')\n",
    "\n",
    "plt.plot(default_samples['param_max_depth'],\n",
    "         default_samples['mean_test_score'],\n",
    "         marker='o', linestyle='-')\n",
    "\n",
    "# Replace -1 with \"None\" in xticks\n",
    "x_ticks = default_samples['param_max_depth'].unique()\n",
    "x_labels = [str(int(x)) if x > 0 else \"None\" for x in x_ticks]\n",
    "\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Effect of max_depth on Model Performance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "final_models.append(('Random Forest', best_rf_model))"
   ],
   "id": "4180282ca9a775ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model 2: AdaBoost",
   "id": "e2dc27061e27bbb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Parameters to tune\n",
    "rates = [0.001, 0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 5.0, 10.0]\n",
    "max_depths = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Initialize tracking variables\n",
    "results = {}\n",
    "best_params = {\"rate\": 0, \"depth\": 0}\n",
    "best_accuracy = 0\n",
    "best_auc = 0\n",
    "\n",
    "# Perform grid search\n",
    "for depth in max_depths:\n",
    "    results[depth] = {\"rates\": [], \"accuracies\": [], \"aucs\": [], \"roc_curves\": []}\n",
    "\n",
    "    for rate in rates:\n",
    "        # Create and train model\n",
    "        ab_clf = AdaBoostClassifier(\n",
    "            DecisionTreeClassifier(max_depth=depth, random_state=42),\n",
    "            n_estimators=50,\n",
    "            learning_rate=rate,\n",
    "            random_state=42,\n",
    "        )\n",
    "        ab_clf.fit(X_train, y_train)\n",
    "\n",
    "        # Get predictions\n",
    "        ab_y_pred = ab_clf.predict(X_test)\n",
    "        ab_y_prob = ab_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, ab_y_pred)\n",
    "        fpr, tpr, _ = roc_curve(y_test, ab_y_prob)\n",
    "        auc = roc_auc_score(y_test, ab_y_prob)\n",
    "\n",
    "        # Store results\n",
    "        results[depth][\"rates\"].append(rate)\n",
    "        results[depth][\"accuracies\"].append(accuracy)\n",
    "        results[depth][\"aucs\"].append(auc)\n",
    "        results[depth][\"roc_curves\"].append((fpr, tpr))\n",
    "\n",
    "        # Update best params if needed\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params[\"rate\"] = rate\n",
    "            best_params[\"depth\"] = depth\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "\n",
    "# Print results\n",
    "print(f\"Best parameters: learning_rate={best_params['rate']}, max_depth={best_params['depth']}\")\n",
    "print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Visualize accuracy results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for depth in max_depths:\n",
    "    plt.plot(rates, results[depth][\"accuracies\"], marker='o', label=f'max_depth={depth}')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('AdaBoost Accuracy vs Learning Rate for Different max_depth Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Visualize ROC curves for best parameters for each max_depth\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for depth in max_depths:\n",
    "    best_idx = np.argmax(results[depth][\"accuracies\"])\n",
    "    best_rate = results[depth][\"rates\"][best_idx]\n",
    "    fpr, tpr = results[depth][\"roc_curves\"][best_idx]\n",
    "    auc = results[depth][\"aucs\"][best_idx]\n",
    "    plt.plot(fpr, tpr, label=f'max_depth={depth}, rate={best_rate}, AUC={auc:.4f}')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Best Parameters')\n",
    "plt.legend()\n",
    "\n",
    "# Create heatmap of accuracies\n",
    "plt.figure(figsize=(12, 8))\n",
    "heatmap_data = np.zeros((len(max_depths), len(rates)))\n",
    "\n",
    "for i, depth in enumerate(max_depths):\n",
    "    for j, rate in enumerate(rates):\n",
    "        idx = results[depth][\"rates\"].index(rate)\n",
    "        heatmap_data[i, j] = results[depth][\"accuracies\"][idx]\n",
    "\n",
    "plt.imshow(heatmap_data, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Accuracy')\n",
    "plt.xticks(np.arange(len(rates)), [str(rate) for rate in rates], rotation=45)\n",
    "plt.yticks(np.arange(len(max_depths)), [str(depth) for depth in max_depths])\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Max Depth')\n",
    "plt.title('Accuracy Heatmap')\n",
    "\n",
    "for i in range(len(max_depths)):\n",
    "    for j in range(len(rates)):\n",
    "        plt.text(j, i, f\"{heatmap_data[i, j]:.4f}\",\n",
    "                 ha=\"center\", va=\"center\", color=\"white\" if heatmap_data[i, j] < 0.8 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ab_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=best_params['depth'], random_state=42),\n",
    "    n_estimators=500,\n",
    "    learning_rate=best_params['rate'],\n",
    "    random_state=42\n",
    ")\n",
    "ab_clf.fit(X_train, y_train)\n",
    "ab_y_prob = ab_clf.predict_proba(X_test)[:, 1]\n",
    "ab_fpr, ab_tpr, _ = roc_curve(y_test, ab_y_prob)\n",
    "ab_auc = roc_auc_score(y_test, ab_y_prob)\n",
    "final_models.append(('AdaBoost', ab_clf))"
   ],
   "id": "53f2da341c64ef6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model 3: Bagging",
   "id": "1c91f975b34b6c8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Parameters to tune\n",
    "max_depths = [1, 2, 3, 4, 5, 10, None]  # None means nodes are expanded until all leaves are pure\n",
    "n_estimators_list = [50, 150, 500]\n",
    "bootstrap_options = [True, False]\n",
    "max_samples_list = [0.5, 0.7, 1.0]  # Proportion of samples to draw\n",
    "\n",
    "# Initialize tracking variables\n",
    "results = {}\n",
    "best_params = {\n",
    "    \"max_depth\": 0,\n",
    "    \"n_estimators\": 0,\n",
    "    \"bootstrap\": False,\n",
    "    \"max_samples\": 0\n",
    "}\n",
    "best_accuracy = 0\n",
    "best_auc = 0\n",
    "\n",
    "# Create a dictionary to store all results for visualization\n",
    "all_results = []\n",
    "\n",
    "# Perform grid search\n",
    "for max_depth in max_depths:\n",
    "    print('*' * 20)\n",
    "    print(f'Trying max depth {max_depth}')\n",
    "    for n_estimators in n_estimators_list:\n",
    "        print(f'Trying {n_estimators=}')\n",
    "        for bootstrap in bootstrap_options:\n",
    "            print(f'Trying {bootstrap=}')\n",
    "            for max_samples in max_samples_list:\n",
    "                # Skip invalid combinations (max_samples only applicable with bootstrap=True)\n",
    "\n",
    "                if not bootstrap and max_samples != 1.0:\n",
    "                    continue\n",
    "                print(f'Trying {max_samples=}')\n",
    "\n",
    "                # Create base estimator\n",
    "                base_estimator = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "\n",
    "                # Create and train model\n",
    "                bag_clf = BaggingClassifier(\n",
    "                    base_estimator,\n",
    "                    n_estimators=n_estimators,\n",
    "                    bootstrap=bootstrap,\n",
    "                    max_samples=max_samples if bootstrap else 1.0,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                bag_clf.fit(X_train, y_train)\n",
    "\n",
    "                # Get predictions\n",
    "                bag_y_pred = bag_clf.predict(X_test)\n",
    "                bag_y_prob = bag_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, bag_y_pred)\n",
    "                try:\n",
    "                    fpr, tpr, _ = roc_curve(y_test, bag_y_prob)\n",
    "                    auc = roc_auc_score(y_test, bag_y_prob)\n",
    "                except:\n",
    "                    fpr, tpr = [0, 1], [0, 1]\n",
    "                    auc = 0.5\n",
    "\n",
    "                # Store results\n",
    "                config = {\n",
    "                    \"max_depth\": max_depth,\n",
    "                    \"n_estimators\": n_estimators,\n",
    "                    \"bootstrap\": bootstrap,\n",
    "                    \"max_samples\": max_samples,\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"auc\": auc,\n",
    "                    \"roc_curve\": (fpr, tpr)\n",
    "                }\n",
    "                all_results.append(config)\n",
    "\n",
    "                # Update best params if needed\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params = {\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"n_estimators\": n_estimators,\n",
    "                        \"bootstrap\": bootstrap,\n",
    "                        \"max_samples\": max_samples\n",
    "                    }\n",
    "\n",
    "                if auc > best_auc:\n",
    "                    best_auc = auc\n",
    "\n",
    "# Print best results\n",
    "print(f\"Best parameters:\")\n",
    "print(f\"  max_depth: {best_params['max_depth']}\")\n",
    "print(f\"  n_estimators: {best_params['n_estimators']}\")\n",
    "print(f\"  bootstrap: {best_params['bootstrap']}\")\n",
    "print(f\"  max_samples: {best_params['max_samples']}\")\n",
    "print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Visualize effect of max_depth with other parameters fixed\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Filter results for bootstrap=True, n_estimators=50, max_samples=1.0\n",
    "filtered_results = [r for r in all_results\n",
    "                   if r[\"bootstrap\"] == True and\n",
    "                      r[\"n_estimators\"] == 50 and\n",
    "                      r[\"max_samples\"] == 1.0]\n",
    "\n",
    "# Sort by max_depth for visualization\n",
    "filtered_results.sort(key=lambda x: str(x[\"max_depth\"]))  # Convert None to string for sorting\n",
    "\n",
    "depth_values = [r[\"max_depth\"] for r in filtered_results]\n",
    "depth_labels = [str(d) for d in depth_values]\n",
    "accuracies = [r[\"accuracy\"] for r in filtered_results]\n",
    "\n",
    "plt.bar(depth_labels, accuracies)\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of Max Depth on Bagging Classifier Accuracy')\n",
    "plt.ylim(min(accuracies) - 0.05, max(accuracies) + 0.05)\n",
    "\n",
    "# Visualize effect of n_estimators\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Filter results for bootstrap=True, max_depth=3, max_samples=1.0\n",
    "filtered_results = [r for r in all_results\n",
    "                   if r[\"bootstrap\"] == True and\n",
    "                      r[\"max_depth\"] == 3 and\n",
    "                      r[\"max_samples\"] == 1.0]\n",
    "\n",
    "# Sort by n_estimators\n",
    "filtered_results.sort(key=lambda x: x[\"n_estimators\"])\n",
    "\n",
    "n_estimators_values = [r[\"n_estimators\"] for r in filtered_results]\n",
    "accuracies = [r[\"accuracy\"] for r in filtered_results]\n",
    "\n",
    "plt.plot(n_estimators_values, accuracies, marker='o')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of Number of Estimators on Bagging Classifier Accuracy')\n",
    "\n",
    "# Visualize bootstrap vs no bootstrap\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Filter results for max_depth=3, n_estimators=50, max_samples=1.0\n",
    "bootstrap_true = [r for r in all_results\n",
    "                 if r[\"max_depth\"] == 3 and\n",
    "                    r[\"n_estimators\"] == 50 and\n",
    "                    r[\"max_samples\"] == 1.0 and\n",
    "                    r[\"bootstrap\"] == True]\n",
    "\n",
    "bootstrap_false = [r for r in all_results\n",
    "                  if r[\"max_depth\"] == 3 and\n",
    "                     r[\"n_estimators\"] == 50 and\n",
    "                     r[\"max_samples\"] == 1.0 and\n",
    "                     r[\"bootstrap\"] == False]\n",
    "\n",
    "labels = ['Bootstrap', 'No Bootstrap']\n",
    "values = [bootstrap_true[0][\"accuracy\"] if bootstrap_true else 0,\n",
    "          bootstrap_false[0][\"accuracy\"] if bootstrap_false else 0]\n",
    "\n",
    "plt.bar(labels, values)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of Bootstrap on Bagging Classifier Accuracy')\n",
    "plt.ylim(min(values) - 0.05, max(values) + 0.05)\n",
    "\n",
    "# Visualize effect of max_samples with bootstrap=True\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Filter results for bootstrap=True, max_depth=3, n_estimators=50\n",
    "filtered_results = [r for r in all_results\n",
    "                   if r[\"bootstrap\"] == True and\n",
    "                      r[\"max_depth\"] == 3 and\n",
    "                      r[\"n_estimators\"] == 50]\n",
    "\n",
    "# Sort by max_samples\n",
    "filtered_results.sort(key=lambda x: x[\"max_samples\"])\n",
    "\n",
    "max_samples_values = [r[\"max_samples\"] for r in filtered_results]\n",
    "accuracies = [r[\"accuracy\"] for r in filtered_results]\n",
    "\n",
    "plt.plot(max_samples_values, accuracies, marker='o')\n",
    "plt.xlabel('Max Samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of Max Samples on Bagging Classifier Accuracy')\n",
    "\n",
    "# Create a summary of top 10 configurations\n",
    "all_results.sort(key=lambda x: x[\"accuracy\"], reverse=True)\n",
    "top_configs = all_results[:10]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "config_labels = [f\"D:{r['max_depth']},N:{r['n_estimators']},\\nB:{r['bootstrap']},S:{r['max_samples']}\" for r in top_configs]\n",
    "accuracies = [r[\"accuracy\"] for r in top_configs]\n",
    "\n",
    "plt.bar(range(len(config_labels)), accuracies)\n",
    "plt.xticks(range(len(config_labels)), config_labels, rotation=45)\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Top 10 Bagging Classifier Configurations')\n",
    "\n",
    "# Visualize ROC curves for top 5 configurations\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for i, config in enumerate(top_configs[:5]):\n",
    "    fpr, tpr = config[\"roc_curve\"]\n",
    "    plt.plot(fpr, tpr,\n",
    "             label=f\"D:{config['max_depth']},N:{config['n_estimators']},B:{config['bootstrap']},S:{config['max_samples']}, AUC:{config['auc']:.4f}\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Top 5 Configurations')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_depth=None, random_state=42),\n",
    "    n_estimators=500,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    max_samples=0.5,\n",
    "    random_state=40\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_y_prob = bag_clf.predict_proba(X_test)[:, 1]\n",
    "bag_fpr, bag_tpr, _ = roc_curve(y_test, bag_y_prob)\n",
    "bag_auc = roc_auc_score(y_test, bag_y_prob)"
   ],
   "id": "755e378bf7146112",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model 4: Support Vector Machines",
   "id": "647885134a694476"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import auc as auc_score\n",
    "from perform_kickstarer_eda import X_train, X_test, y_train, y_test\n",
    "# TODO: REMOVE, ITS ALREADY ABOVEW\n",
    "\n",
    "def evaluate_svc_models_rapids(models, X, y):\n",
    "    results = []\n",
    "    names = []\n",
    "\n",
    "    # Convert to GPU if needed (assuming X and y are numpy arrays)\n",
    "    X_gpu = cudf.DataFrame(X) if not isinstance(X, cudf.DataFrame) else X\n",
    "    y_gpu = cudf.Series(y) if not isinstance(y, cudf.Series) else y\n",
    "\n",
    "    for name, _ in models:\n",
    "        print(f\"Tuning hyperparameters for: {name}\")\n",
    "        kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "        if name == 'SVC_RBF':\n",
    "            param_grid = {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'gamma': ['scale', 'auto', 0.01, 0.1]\n",
    "            }\n",
    "            # cuML SVC doesn't use 'probability' parameter like sklearn\n",
    "            model = cuSVC(kernel='rbf')\n",
    "        elif name == 'SVC_POLY':\n",
    "            param_grid = {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'degree': [2, 3, 4],\n",
    "                'gamma': ['scale', 'auto']\n",
    "            }\n",
    "            model = cuSVC(kernel='poly')\n",
    "        elif name == 'SVC_LINEAR':\n",
    "            param_grid = {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'tol': [1e-4, 1e-3]\n",
    "            }\n",
    "            model = cuSVC(kernel='linear')\n",
    "        elif name == 'SVC_SIGMOID':\n",
    "            param_grid = {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'gamma': ['scale', 'auto']\n",
    "            }\n",
    "            model = cuSVC(kernel='sigmoid')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Manual grid search for cuML SVC (since GridSearchCV isn't directly compatible)\n",
    "        best_score = 0\n",
    "        best_params = {}\n",
    "        best_model = None\n",
    "\n",
    "        # Generate parameter combinations\n",
    "        param_combinations = []\n",
    "        for param_name, param_values in param_grid.items():\n",
    "            if not param_combinations:\n",
    "                param_combinations = [{param_name: value} for value in param_values]\n",
    "            else:\n",
    "                new_combinations = []\n",
    "                for combo in param_combinations:\n",
    "                    for value in param_values:\n",
    "                        new_combo = combo.copy()\n",
    "                        new_combo[param_name] = value\n",
    "                        new_combinations.append(new_combo)\n",
    "                param_combinations = new_combinations\n",
    "\n",
    "        # Perform manual grid search with cross-validation\n",
    "        for params in param_combinations:\n",
    "            current_model = cuSVC(kernel=model.kernel, **params)\n",
    "            fold_scores = []\n",
    "\n",
    "            for train_idx, val_idx in kfold.split(X):\n",
    "                # Extract folds (using numpy indices works with cuDF)\n",
    "                X_train_fold = X_gpu.iloc[train_idx]\n",
    "                y_train_fold = y_gpu.iloc[train_idx]\n",
    "                X_val_fold = X_gpu.iloc[val_idx]\n",
    "                y_val_fold = y_gpu.iloc[val_idx]\n",
    "\n",
    "                # Train and evaluate\n",
    "                current_model.fit(X_train_fold, y_train_fold)\n",
    "                y_pred = current_model.predict(X_val_fold)\n",
    "                score = accuracy_score(y_val_fold, y_pred)\n",
    "                fold_scores.append(score)\n",
    "\n",
    "            # Calculate mean score across folds\n",
    "            mean_score = sum(fold_scores) / len(fold_scores)\n",
    "\n",
    "            # Update best model if this one is better\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = params\n",
    "                best_model = current_model\n",
    "\n",
    "        # Train best model on full dataset\n",
    "        best_model = cuSVC(kernel=model.kernel, **best_params)\n",
    "        best_model.fit(X_gpu, y_gpu)\n",
    "\n",
    "        print(f\"Best params for {name}: {best_params}\")\n",
    "        print(f\"Best CV accuracy: {best_score:.4f}\")\n",
    "\n",
    "        # For metrics, convert predictions back to numpy for sklearn compatibility\n",
    "        # Manual cross-validation for F1 score\n",
    "        y_pred_list = []\n",
    "        y_true_list = []\n",
    "\n",
    "        # Temporary workaround for ROC since cuML SVC doesn't have predict_proba\n",
    "        # We'll use decision_function instead which gives distance from hyperplane\n",
    "        decision_scores_list = []\n",
    "\n",
    "        for train_idx, val_idx in kfold.split(X):\n",
    "            X_train_fold = X_gpu.iloc[train_idx]\n",
    "            y_train_fold = y_gpu.iloc[train_idx]\n",
    "            X_val_fold = X_gpu.iloc[val_idx]\n",
    "            y_val_fold = y_gpu.iloc[val_idx]\n",
    "\n",
    "            fold_model = cuSVC(kernel=model.kernel, **best_params)\n",
    "            fold_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            y_pred = fold_model.predict(X_val_fold)\n",
    "\n",
    "            # Use decision_function instead of predict_proba for ROC curve\n",
    "            decision_scores = fold_model.decision_function(X_val_fold)\n",
    "\n",
    "            # Handle different return types (cuDF Series/DataFrame or numpy array)\n",
    "            if hasattr(y_pred, 'to_numpy'):\n",
    "                y_pred_list.extend(y_pred.to_numpy())\n",
    "            else:\n",
    "                y_pred_list.extend(y_pred)\n",
    "\n",
    "            if hasattr(y_val_fold, 'to_numpy'):\n",
    "                y_true_list.extend(y_val_fold.to_numpy())\n",
    "            else:\n",
    "                y_true_list.extend(y_val_fold)\n",
    "\n",
    "            if hasattr(decision_scores, 'to_numpy'):\n",
    "                decision_scores_list.extend(decision_scores.to_numpy())\n",
    "            else:\n",
    "                decision_scores_list.extend(decision_scores)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(np.array(y_true_list), np.array(y_pred_list))\n",
    "        print(f\"F1 Score for {name}: {f1:.4f}\")\n",
    "\n",
    "        # ROC Curve using decision function scores instead of probabilities\n",
    "        fpr, tpr, _ = roc_curve(np.array(y_true_list), np.array(decision_scores_list))\n",
    "        roc_auc = auc_score(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "        # Save results for boxplot\n",
    "        cv_results = np.array(fold_scores)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "\n",
    "        print(f\"F1 Score: {f1:.4f} | AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Plot results\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Tuned cuML SVC Model Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(results, labels=names)\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    return results, names\n",
    "\n",
    "# Define models\n",
    "models = [('SVC_RBF', None), ('SVC_POLY', None), ('SVC_LINEAR', None), ('SVC_SIGMOID', None)]\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_svc_models_rapids(models, X_train, y_train)"
   ],
   "id": "7d19cc33219e49ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model 5: Multilayer Perceptron",
   "id": "27c1ebe07a8a5368"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "configs = [\n",
    "    {\"layers\": [16], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [16], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [32], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [32], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [64], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [64], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [128], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [128], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [256], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [256], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [32, 16], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [32, 16], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [64, 32], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [64, 32], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [128, 64], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [128, 64], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [64, 32, 16], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [64, 32, 16], \"activation\": \"relu\", \"optimizer\": \"adam\", \"learning_rate\": 0.01},\n",
    "\n",
    "    {\"layers\": [16], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [16], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [32], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [32], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [64], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [64], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [128], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [128], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [256], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [256], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [32, 16], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [32, 16], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [64, 32], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [64, 32], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [128, 64], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [128, 64], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.01},\n",
    "    {\"layers\": [64, 32, 16], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.001},\n",
    "    {\"layers\": [64, 32, 16], \"activation\": \"relu\", \"optimizer\": \"sgd\", \"learning_rate\": 0.01}\n",
    "]\n",
    "\n",
    "results = []\n",
    "for config in configs:\n",
    "    print(f\"Trying config: {config}\")\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(config[\"learning_rate\"])\n",
    "    elif config[\"optimizer\"] == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=config[\"learning_rate\"])\n",
    "\n",
    "    # Build model with current configuration\n",
    "    input_layer = tf.keras.layers.Input(shape=(X_train.shape[1],))\n",
    "    x = input_layer\n",
    "    for units in config[\"layers\"]:\n",
    "        x = tf.keras.layers.Dense(units, activation=config[\"activation\"])(x)\n",
    "\n",
    "    # Binary classification output\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Get best results based on validation accuracy\n",
    "    val_acc_hist = history.history[\"val_accuracy\"]\n",
    "    best_val_acc = np.max(val_acc_hist)\n",
    "    best_epoch = np.argmax(val_acc_hist)\n",
    "\n",
    "    best_loss = history.history[\"loss\"][best_epoch]\n",
    "    best_acc = history.history[\"accuracy\"][best_epoch]\n",
    "    best_val_loss = history.history[\"val_loss\"][best_epoch]\n",
    "    best_auc = history.history[\"auc\"][best_epoch]\n",
    "    best_val_auc = history.history[\"val_auc\"][best_epoch]\n",
    "\n",
    "    print(f\"Best epoch: {best_epoch+1} | loss: {best_loss:.4f} - acc: {best_acc:.4f} - auc: {best_auc:.4f} - val_loss: {best_val_loss:.4f} - val_acc: {best_val_acc:.4f} - val_auc: {best_val_auc:.4f}\")\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"config\": config,\n",
    "        \"epoch\": best_epoch+1,\n",
    "        \"loss\": best_loss,\n",
    "        \"accuracy\": best_acc,\n",
    "        \"auc\": best_auc,\n",
    "        \"val_loss\": best_val_loss,\n",
    "        \"val_accuracy\": best_val_acc,\n",
    "        \"val_auc\": best_val_auc,\n",
    "        \"history\": history.history\n",
    "    })\n",
    "\n",
    "# Display results table\n",
    "df_results = DataFrame(results)\n",
    "display(df_results)\n",
    "\n",
    "# Select the best model configuration based on validation accuracy\n",
    "best_idx = df_results[\"val_accuracy\"].idxmax()\n",
    "best_config = df_results.loc[best_idx, \"config\"]\n",
    "print(f\"Best configuration: {best_config}\")\n",
    "\n",
    "# Build final model with best configuration\n",
    "tf.keras.backend.clear_session()\n",
    "input_layer = tf.keras.layers.Input(shape=(X_train.shape[1],))\n",
    "x = input_layer\n",
    "for units in best_config[\"layers\"]:\n",
    "    x = tf.keras.layers.Dense(units, activation=best_config[\"activation\"])(x)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "final_model = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "final_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\", tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# Train the final model\n",
    "final_history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "# Plot learning curves for final model\n",
    "DataFrame(final_history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Evaluate the final model\n",
    "test_loss, test_acc, test_auc, test_precision, test_recall = final_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_prob = final_model.predict(X_test).flatten()\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc_score(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "final_models.append(('MLP', final_model))"
   ],
   "id": "f1c9ed5eb5e52658",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model 6: Decision Tree",
   "id": "467c41e28da40012"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the optimal features we found\n",
    "optimal_features = ['backers_count', 'goal', 'launch_to_state_change_days', 'create_to_launch_days', 'launch_to_deadline_days', 'created_at_day', 'blurb_len_clean', 'created_at_hr', 'launched_at_day', 'name_len_clean', 'launched_at_hr', 'state_changed_at_day', 'deadline_day', 'created_at_month', 'state_changed_at_hr', 'deadline_hr', 'launched_at_month', 'state_changed_at_month', 'category_Gadgets', 'deadline_month', 'category_Software', 'category_Musical', 'category_Hardware', 'category_Apps', 'category_None', 'category_Web', 'created_at_weekday_Friday', 'staff_pick', 'created_at_weekday_Wednesday', 'deadline_weekday_Thursday']\n",
    "\n",
    "# Train the model using optimal features\n",
    "X_train_optimal = X_train[optimal_features]\n",
    "X_test_optimal = X_test[optimal_features]\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_depth': range(1, 21, 2),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train_optimal, y_train)\n",
    "\n",
    "# Get best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_dt_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "y_pred = best_dt_model.predict(X_test_optimal)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature importance of the final model\n",
    "feature_names = best_dt_model.feature_importances_\n",
    "importance_df = DataFrame({\n",
    "    'Feature': optimal_features,\n",
    "    'Importance': feature_names\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(importance_df.head(10))  # Show top 10 features\n",
    "\n",
    "# Calculate cross-validation score for the best model\n",
    "cv_scores = cross_val_score(\n",
    "    best_dt_model,\n",
    "    X[optimal_features], Y1,\n",
    "    cv=5, scoring='accuracy'\n",
    ")\n",
    "print(f\"\\nCross-validation accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_tree(best_dt_model, filled=True,\n",
    "          feature_names=feature_names,\n",
    "          precision=2, proportion=True)\n",
    "plt.title(f\"Classifier Decision Tree: Depth {best_dt_model.get_depth()}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Plot ROC curve\n",
    "y_scores = best_dt_model.predict_proba(X_test_optimal)[:, 1]\n",
    "fpr, tpr, thresholds2 = roc_curve(y_test, y_scores)\n",
    "roc_auc = auc_score(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2,\n",
    "         label=f'ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize hyperparameter tuning results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Filter results for default min_samples values\n",
    "default_samples = results[(results['param_min_samples_split'] == 2) &\n",
    "                         (results['param_min_samples_leaf'] == 1)]\n",
    "\n",
    "# Convert None to -1 for plotting purposes\n",
    "default_samples['param_max_depth'] = default_samples['param_max_depth'].fillna(-1)\n",
    "\n",
    "# Sort by max_depth for proper x-axis order\n",
    "default_samples = default_samples.sort_values('param_max_depth')\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(default_samples['param_max_depth'],\n",
    "         default_samples['mean_test_score'],\n",
    "         marker='o', linestyle='-')\n",
    "\n",
    "# Replace -1 with \"None\" in xticks\n",
    "x_ticks = default_samples['param_max_depth'].unique()\n",
    "x_labels = [str(int(x)) if x > 0 else \"None\" for x in x_ticks]\n",
    "plt.xticks(x_ticks, x_labels)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Effect of max_depth on Decision Tree Performance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "final_models.append(('Decision Tree', best_dt_model))"
   ],
   "id": "3d1bc223919b1897",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 3: Display results (from all final models)",
   "id": "65d11203b6562b07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for name, model in final_models:\n",
    "    print(f\"Model: {name}\")\n",
    "    # TODO: Display results for each model"
   ],
   "id": "3193ef527dfa0e2f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
